# eks
# curl -LO https://dl.k8s.io/release/v1.18.0/bin/linux/amd64/kubectl
# aws eks --region "eu-central-1" update-kubeconfig --name "test-eks-cluster-1"
# terraform output ssh_private_key_pem > ../keys/sshKey/ssh-key.pem

#outputs
output "cluster_id" {
  description = "EKS cluster ID."
  value       = module.eks.cluster_id
}

output "cluster_endpoint" {
  description = "Endpoint for EKS control plane."
  value       = module.eks.cluster_endpoint
}

output "cluster_security_group_id" {
  description = "Security group ids attached to the cluster control plane."
  value       = module.eks.cluster_security_group_id
}

output "kubectl_config" {
  description = "kubectl config as generated by the module."
  value       = module.eks.kubeconfig
}

output "config_map_aws_auth" {
  description = "A kubernetes configuration to authenticate to this EKS cluster."
  value       = module.eks.config_map_aws_auth
}

output "cluster_name" {
  description = "Kubernetes Cluster Name"
  value       = var.cluster_name
}


variable "public_ip" {
  description = "map ips to worker"
  type        = string
  default     = "true"
}

variable "cluster_name" {
  type        = string
  description = "EKS cluster name."
  default = "test-eks-cluster-1"
}

# data
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_id
}

data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_id
}

# route-table to route private subnet to public subnet by natting
resource "aws_route_table" "rt_eks_private_dev_main" {
  vpc_id = aws_vpc.vpc_dev_main.id
  route {
    cidr_block = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.nat_gateway.id
  }
  tags = {
    Name          = "eks private subnet rt"
    "environment" = "development"
  }
}


#subnets
resource "aws_subnet" "eks-subnet-1_private_dev_main" {
  vpc_id            = aws_vpc.vpc_dev_main.id
  cidr_block        = "10.0.3.0/24"
  availability_zone = "${var.region}${var.availability-zone}"
  map_public_ip_on_launch = "true" #var.public_ip
  tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"           = "1"
    iac_environment                             = "development"
  }
}

resource "aws_route_table_association" "rt-association_private_subnet_eks_1_dev_main" {
  subnet_id      = aws_subnet.eks-subnet-1_private_dev_main.id
  route_table_id = aws_route_table.rt_eks_private_dev_main.id
}

resource "aws_subnet" "eks-subnet-2_private_dev_main" {
  vpc_id            = aws_vpc.vpc_dev_main.id
  cidr_block        = "10.0.4.0/24"
  availability_zone = "${var.region}${var.availability-zone_second}"
  map_public_ip_on_launch = "true" # var.public_ip
  tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"           = "1"
    iac_environment                             = "development"
  }
}

resource "aws_route_table_association" "rt-association_private_subnet_eks_2_dev_main" {
  subnet_id      = aws_subnet.eks-subnet-2_private_dev_main.id
  route_table_id = aws_route_table.rt_eks_private_dev_main.id
}

resource "aws_subnet" "eks-subnet-2_public_dev_main" {
  vpc_id            = aws_vpc.vpc_dev_main.id
  cidr_block        = "10.0.5.0/24"
  availability_zone = "${var.region}${var.availability-zone_second}"
  map_public_ip_on_launch = "true" # var.public_ip
  tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/elb"           = "1"
    iac_environment                             = "development"
  }
}

resource "aws_route_table_association" "rt-association_public_subnet_eks_2_dev_main" {
  subnet_id      = aws_subnet.eks-subnet-2_public_dev_main.id
  route_table_id = aws_route_table.rt_public_dev_main.id
}


# secruity group (sg)
resource "aws_security_group" "worker_group_mgmt_one" {
  name        = "secruity-group-ek-main"
  vpc_id      = aws_vpc.vpc_dev_main.id

  ingress {
    from_port = 22
    to_port   = 22
    protocol  = "tcp"

    # https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html
    # You can grant access to a specific CIDR range, or to another security group in your VPC or in a peer VPC (requires a VPC peering connection).
    # source can be another security group, an IPv4 or IPv6 CIDR block, a single IPv4 or IPv6 address, or a prefix list ID.
    # destination can be another security group, an IPv4 or IPv6 CIDR block, a single IPv4 or IPv6 address, or a prefix list ID.
    cidr_blocks = ["10.0.0.0/16"]
    }
}

resource "aws_security_group" "all_worker_mgmt" {
  vpc_id      = aws_vpc.vpc_dev_main.id

  ingress {
    from_port = 0
    to_port   = 0
    protocol  = "-1"

    cidr_blocks = [
      "10.0.0.0/16",
      "172.16.0.0/12",
      "192.168.0.0/16",
    ]
  }

  egress {
    from_port = 0
    to_port   = 0
    protocol  = "-1"

    cidr_blocks = [
      "10.0.0.0/16",
      "172.16.0.0/12",
      "192.168.0.0/16",
    ]
  }
}


# kubernetes provider
# get EKS authentication for being able to manage k8s objects from terraform
# otherwise, it throws an error when creating `kubernetes_config_map.aws_auth`.
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}


module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  version = "14.0.0"
  cluster_name    = var.cluster_name
  cluster_version = "1.18"
  create_eks = true
  write_kubeconfig = false

  vpc_id = aws_vpc.vpc_dev_main.id
  subnets         = [aws_subnet.subnet_public.id, aws_subnet.eks-subnet-2_public_dev_main.id]
  worker_additional_security_group_ids = [
    aws_security_group.all_worker_mgmt.id
  ]
  
  
  # defaults for each launch-template
  # workers_group_defaults = {
  #   key_name               = aws_key_pair.ssh.key_name
  #   root_volume_size       = 10
  #   asg_recreate_on_change = true
  #   root_volume_type = "gp2"
  #   additional_userdata    = <<EOF
  #     #!/bin/bash
  #     echo "hello world"
  #   EOF
  #   public_ip = true # var.public_ip
  # }
  
  # the number of launch_templates has to be 2 (or more idk)
  # i guess the number has to match the number of given subnets (here 2)
  # why? idk
  worker_groups_launch_template = [
    {
      name                 = "worker-group-3"
      # i guess you cant use default ubuntu or something like that
      # since autoscaling needs a os which has e.g. kubectl installed by default
      # if you leave this parameter commented out (like here) terraform looks for latest aws-eks-optimized image
      # for this image you can also specify "bootstrap_extra_args"
      # ami_id = "ami-0502e817a62226e03"
      instance_type = "t3.small"
      asg_desired_capacity = 2
      asg_min_size            = 1
      asg_max_size            = 3
      bootstrap_extra_args    = "--use-max-pods false"
      public_ip            = true
    },
    {
      name                 = "worker-group-4"
      # ami_id = "ami-0502e817a62226e03"
      instance_type = "t3.small"
      asg_desired_capacity = 2
      asg_min_size            = 1
      asg_max_size            = 3
      bootstrap_extra_args    = "--use-max-pods false"
      public_ip            = true
    },
  ]

  #   worker_groups_launch_template = [
  #   {
  #     name                 = "worker-group-1"
  #     instance_type        = "t3.small"
  #     asg_desired_capacity = 2
  #     public_ip            = true
  #   },
  #   {
  #     name                 = "worker-group-2"
  #     instance_type        = "t3.medium"
  #     asg_desired_capacity = 1
  #     public_ip            = true
  #   },
  # ]

  tags = {
    Environment = "development"
  }
}